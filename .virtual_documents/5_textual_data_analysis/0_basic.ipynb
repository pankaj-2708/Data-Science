import pandas as pd
import numpy as np


#optimization things

from tqdm.notebook import tqdm
tqdm.pandas()


df=pd.read_csv("IMDB Dataset.csv")
print(df.shape)

df=df.sample(100)
df.head()



df.shape


# we are going to perform steps on this dataset to make it ready for applying in model

# we will be doing

# data cleaning
# data preprocessing
# EDA
# making new features
# vectorization


df.isnull().sum()


df.duplicated().sum()


df.drop_duplicates(inplace=True)


df.duplicated().sum()


df.info()





# converting to lowercase

df["review"]=df["review"].str.lower()


# removing extra white spaces

df["review"]=df["review"].str.strip()


# removing html
import re

def remove_html(st):
    clean=re.compile("<.*?>")
    return re.sub(clean,"",st)

df["review"]=df["review"].apply(remove_html)


# removing urls

def remove_url(st):
    clean=re.compile("https?://\S+|www\.\S+")
    return re.sub(clean,"",st)

df["review"]=df["review"].apply(remove_url)


# removing aposhtrophes

contractions = [
    "i'm",
    "you're",
    "he's",
    "she's",
    "it's",
    "we're",
    "they're",
    "i've",
    "you've",
    "we've",
    "they've",
    "i'd",
    "you'd",
    "he'd",
    "she'd",
    "we'd",
    "they'd",
    "i'll",
    "you'll",
    "he'll",
    "she'll",
    "we'll",
    "they'll",
    "isn't",
    "aren't",
    "wasn't",
    "weren't",
    "haven't",
    "hasn't",
    "hadn't",
    "don't",
    "doesn't",
    "didn't",
    "can't",
    "couldn't",
    "won't",
    "wouldn't",
    "shouldn't",
    "mustn't",
    "what's",
    "where's",
    "when's",
    "who's",
    "how's",
    "that's",
    "there's",
    "y'all"
]

explanations=['i am',
 'you are',
 'he is ',
 'she is ',
 'it is ',
 'we are',
 'they are',
 'i have',
 'you have',
 'we have',
 'they have',
 'i would ',
 'you would ',
 'he would ',
 'she would ',
 'we would ',
 'they would ',
 'i will',
 'you will',
 'he will',
 'she will',
 'we will',
 'they will',
 'is not',
 'are not',
 'was not',
 'were not',
 'have not',
 'has not',
 'had not',
 'do not',
 'does not',
 'did not',
 'cannot',
 'could not',
 'will not',
 'would not',
 'should not',
 'must not',
 'what is ',
 'where is ',
 'when is ',
 'who is ',
 'how is ',
 'that is',
 'there is ',
 'you all']

for i in range(len(contractions)):
    df["review"]=df["review"].str.replace(contractions[i],explanations[i])


df.head()


!pip install symspellpy
!wget https://raw.githubusercontent.com/wolfgarbe/SymSpell/master/SymSpell/frequency_dictionary_en_82_765.txt


df.iloc[1,0]=" Thiss is an exmple of badd text"


import language_tool_python
from tqdm import tqdm
tqdm.pandas()

tool = language_tool_python.LanguageTool('en-US')

def correct_with_languagetool(text):
    matches = tool.check(text)
    return language_tool_python.utils.correct(text, matches)

# Apply to DataFrame (test on sample first)
df["review_corrected"] = df["review"].progress_apply(correct_with_languagetool)



df.head()


# removing punctutation
import string
punctuation=list(string.punctuation)

def removePunc(text):
    for i in range(len(punctuation)):
        text=text.replace(punctuation[i],"")
    return text
    
df["review"]=df["review"].apply(removePunc)


# removing special charecters

def removeSC(st):
    clean=re.compile("\W\s")
    return re.sub(clean,"",st)

df["review"]=df["review"].apply(removeSC)





# tokenization 
import nltk
nltk.download('punkt_tab')
from nltk.tokenize import word_tokenize

df["tokenized_review"]=df["review"].progress_apply(word_tokenize)


# stop word removal

from nltk.corpus import stopwords
import nltk
nltk.download('stopwords')
all_stopwords=stopwords.words("english")

def remove_stopWord(lst):
    L=[]
    for word in lst:
        if word not in all_stopwords:
            L.append(word)
    return L


df["tokenized_review"]=df["tokenized_review"].apply(remove_stopWord)


df["review"]=df["tokenized_review"].apply(lambda x:" ".join(x))





df["char_length"]=df["review"].str.len()
df["word_length"]=df["tokenized_review"].apply(len)


import seaborn as sns

sns.distplot(df[df["sentiment"]=="positive"]["word_length"],hist=False)
sns.distplot(df[df["sentiment"]=="negative"]["word_length"],hist=False)
# if there is diff in distribution then we can say that it will contribute to increase accuracy 
# of mode



sns.distplot(df[df["sentiment"]=="positive"]["char_length"],hist=False)
sns.distplot(df[df["sentiment"]=="negative"]["char_length"],hist=False)


# above create features are not seemed to be very useful so droping them


#  we can check in a list of negative/positive words how many time it come

posNeg=pd.read_excel("Positive and Negative Word List.xlsx")
negative_wrod_list=posNeg["Negative Sense Word List"].values
positive_wrod_list=posNeg["Positive Sense Word List"].values

negative_wrod_list=[i for i in negative_wrod_list if type(i)==str]
positive_wrod_list=[i for i in positive_wrod_list if type(i)==str]

def countNegative(lst):
    count=0
    for i in lst:
        if i in negative_wrod_list:
            count+=1
    return count
    
def countPositive(lst):
    count=0
    for i in lst:
        if i in positive_wrod_list:
            count+=1
    return count

df["PositiveWordCount"]=df["tokenized_review"].progress_apply(countPositive)
df["NegativeWordCount"]=df["tokenized_review"].progress_apply(countNegative)


# n grams

# ngrams means dividing into groups of n

# eg - my name is pankaj 

# applying 2 grams

# my name , name is , is pankaj 


from nltk import ngrams


pd.Series(ngrams(df[df["sentiment"]=="negative"]["tokenized_review"].sum(),2)).value_counts()


pd.Series(ngrams(df[df["sentiment"]=="positive"]["tokenized_review"].sum(),2)).value_counts()


# creating wordcloud 
from wordcloud import WordCloud
import matplotlib.pyplot as plt

plt.figure(figsize=(20,20))
wc=WordCloud(width=1600, height=800).generate(" ".join(df[df["sentiment"]=='negative']['review']))
plt.figure(figsize=(20, 10))
plt.imshow(wc, interpolation='bilinear')
plt.axis('off')
plt.title("Word Cloud for Negative Reviews", fontsize=20)
plt.show()


plt.figure(figsize=(20,20))
wc=WordCloud(width=1600, height=800).generate(" ".join(df[df["sentiment"]=='positive']['review']))
plt.figure(figsize=(20, 10))
plt.imshow(wc, interpolation='bilinear')
plt.axis('off')
plt.title("Word Cloud for Negative Reviews", fontsize=20)
plt.show()





# i am going to use bag of words for vectorization

from sklearn.feature_extraction.text import CountVectorizer

count_vec=CountVectorizer(max_features=5000,ngram_range=(1,2))
# it will select top 5000 words among unigram and bigram beacuse n_gram_range is from till 2

bag_of_words=count_vec.fit_transform(df['review'])
bag_of_words=pd.DataFrame(bag_of_words.toarray(),columns=count_vec.get_feature_names_out())


df.reset_index(inplace=True,drop=True)
df=pd.concat([df,bag_of_words],axis=1)


df.head()





df.drop(['review','tokenized_review'],axis=1,inplace=True)


from sklearn.preprocessing import LabelEncoder
df["sentiment"]=le.fit_transform(df["sentiment"])


df.head()


from sklearn.model_selection import train_test_split

X_train,X_test,y_train,y_test=train_test_split(df.iloc[:,1:].values,df.iloc[:,0].values,test_size=0.1)



from sklearn.linear_model import LogisticRegression

lg=LogisticRegression(max_iter=5000)

lg.fit(X_train,y_train)

y_pred=lg.predict(X_test)


from sklearn.metrics import accuracy_score

accuracy_score(y_test,y_pred)



